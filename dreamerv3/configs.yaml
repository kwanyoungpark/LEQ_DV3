defaults:
  algo: BC
  seed: 0
  method: name
  task: dummy_disc
  logdir: /dev/null
  eval_dir: ''
  #filter: 'score|length|fps|ratio|train/.*_loss$|train/rand/.*/mean'
  filter: 'score|length|fps|ratio|train/.*_loss$|train/rand/.*/mean|report/.*/mean|eval/.*/mean'
  tensorboard_videos: True

  replay:
    size: 5e6
    online: False
    fracs: {uniform: 1.0, priority: 0.0, recency: 0.0}
    prio: {exponent: 0.8, maxfrac: 0.5, initial: inf, zero_on_sample: True}
    priosignal: model
    recexp: 1.0
    chunksize: 1024
    save_wait: False

  jax:
    platform: gpu
    jit: True
    compute_dtype: bfloat16
    #compute_dtype: float32
    param_dtype: float32
    prealloc: True
    checks: False
    logical_cpus: 0
    debug: False
    policy_devices: [0]
    train_devices: [0]
    sync_every: 1
    profiler: False
    transfer_guard: True
    assert_num_devices: -1
    fetch_policy_carry: False
    nvidia_flags: False
    xla_dump: False

  run:
    script: train_eval
    steps: 1e10
    duration: 0
    num_envs: 10 #16
    num_envs_eval: 10 #4
    expl_until: 0
    log_every: 120
    save_every: 900
    eval_every: 2500
    eval_initial: True
    eval_eps: 10 # 1
    train_fill: 0
    eval_fill: 0
    log_zeros: True
    log_keys_video: [image,vector]
    log_keys_sum: '^$'
    log_keys_avg: '^$'
    log_keys_max: '^$'
    log_video_fps: 20
    log_video_streams: 10 #4
    log_episode_timeout: 60
    from_checkpoint: ''
    model_checkpoint: ''
    actor_addr: 'tcp://localhost:{auto}'
    replay_addr: 'tcp://localhost:{auto}'
    logger_addr: 'tcp://localhost:{auto}'
    actor_batch: 8
    actor_threads: 4
    env_replica: -1
    ipv6: False
    usage: {psutil: True, nvsmi: True, gputil: False, malloc: False, gc: False}
    timer: True
    driver_parallel: True
    agent_process: False
    remote_replay: False

  wrapper: {length: 0, reset: True, discretize: 0, checks: True}
  env:
    #atari: {size: [64, 64], repeat: 4, sticky: True, gray: True, actions: all, lives: unused, noops: 0, autostart: False, pooling: 2, aggregate: max, resize: pillow}
    atari: {size: [64, 64], repeat: 4, sticky: False, gray: True, actions: needed, lives: unused, noops: 0, autostart: False, pooling: 2, aggregate: max, resize: pillow}
    atari1p: {size: [64, 64], repeat: 4, sticky: False, gray: True, actions: needed, lives: unused, noops: 0, autostart: False, pooling: 2, aggregate: max, resize: pillow}
    atari5p: {size: [64, 64], repeat: 4, sticky: False, gray: True, actions: needed, lives: unused, noops: 0, autostart: False, pooling: 2, aggregate: max, resize: pillow}
    crafter: {size: [64, 64], logs: False, use_logdir: False}
    atari100k: {size: [64, 64], repeat: 4, sticky: False, gray: False, actions: all, lives: unused, noops: 0, autostart: False, resize: pillow, length: 100000}
    dmlab: {size: [64, 64], repeat: 4, episodic: True, actions: popart, use_seed: False}
    minecraft: {size: [64, 64], break_speed: 100.0, logs: False}
    dmc: {size: [64, 64], repeat: 2, image: True, camera: -1}
    procgen: {size: [64, 64]}
    loconav: {size: [64, 64], repeat: 2, camera: -1}

  # Agent
  report: True
  report_gradnorms: False
  batch_size: 16 #64
  batch_length: 65
  batch_length_eval: 64 
  replay_length: 0
  replay_length_eval: 0
  #replay_context: 1
  replay_context: False
  random_agent: False
  loss_scales: {dec_cnn: 1.0, dec_mlp: 1.0, reward: 1.0, cont: 1.0, dyn: 1.0, rep: 0.1, reg: 1.0, actor: 1.0, actor_bc: 1.0, value: 1.0, critic: 1.0, replay_critic: 1.0, G: 0.0, D: 0.0}
  #loss_scales: {dec_cnn: 0.0, dec_mlp: 0.0, reward: 0.0, cont: 0.0, dyn: 0.0, rep: 0.0, reg: 0.0, actor: 1.0, value: 0.0, critic: 0.0, replay_critic: 0.0}
  opt: {scaler: rms, lr: 4e-5, eps: 1e-20, momentum: True, wd: 0.0, warmup: 1000, globclip: 0.0, agc: 0.3, beta1: 0.9, beta2: 0.999, details: False, pmin: 1e-3, anneal: 0, schedule: constant}
  separate_lrs: False
  lrs: {dec: 1e-4, enc: 1e-4, dyn: 1e-4, rew: 1e-4, con: 1e-4, actor: 3e-5, critic: 3e-5, value: 3e-5}
  ac_grads: none
  #ac_grads: all
  reset_context: 0.0
  replay_critic_loss: onestep
  replay_critic_grad: True
  replay_critic_bootstrap: imag
  reward_grad: True
  report_openl_context: 8

  # World Model
  dyn:
    typ: rssm
    rssm: {deter: 8192, hidden: 1024, stoch: 32, classes: 64, act: silu, norm: rms, unimix: 0.01, outscale: 1.0, winit: normal, imglayers: 2, obslayers: 1, dynlayers: 1, absolute: False, cell: blockgru, blocks: 8, block_fans: False, block_norm: False, ensemble: 7, markov: False, actless: False}
  enc:
    spaces: '.*'
    typ: simple
    simple: {depth: 64, mults: [1, 2, 3, 4, 4], layers: 3, units: 1024, act: silu, norm: rms, winit: normal, symlog: True, outer: True, kernel: 5, minres: 4}
  dec:
    spaces: '.*'
    typ: simple
    simple: {inputs: [deter, stoch], vecdist: symlog_mse, depth: 64, mults: [1, 2, 3, 4, 4], layers: 3, units: 1024, act: silu, norm: rms, outscale: 1.0, winit: normal, outer: True, kernel: 5, minres: 4, block_space: 8, block_fans: False, block_norm: False, hidden_stoch: True, space_hidden: 0}
  rewhead: {layers: 1, units: 1024, act: silu, norm: rms, dist: symexp_twohot, outscale: 0.0, inputs: [deter, stoch], winit: normal, bins: 255, block_fans: False, block_norm: False}
  #rewhead: {layers: 1, units: 1024, act: silu, norm: rms, dist: mse, outscale: 0.0, inputs: [deter, stoch], winit: normal, bins: 255, block_fans: False, block_norm: False}
  conhead: {layers: 1, units: 1024, act: silu, norm: rms, dist: binary, outscale: 1.0, inputs: [deter, stoch], winit: normal, block_fans: False, block_norm: False}
  dcrhead: {layers: 1, units: 1024, act: silu, norm: rms, dist: binary, outscale: 1.0, inputs: [tensor], winit: normal, block_fans: False, block_norm: False}
  contdisc: True
  rssm_loss: {free: 1.0}

  # Actor Critic
  expectile: 0.1
  plan: False
  mopo_weight: 3.0
  actor: {layers: 3, units: 1024, act: silu, norm: rms, minstd: 0.1, maxstd: 1.0, outscale: 0.01, unimix: 0.01, inputs: [deter, stoch], winit: normal, block_fans: False, block_norm: False}
  critic: {layers: 3, units: 1024, act: silu, norm: rms, dist: symexp_twohot, outscale: 0.0, inputs: [deter, stoch, action], winit: normal, bins: 255, block_fans: False, block_norm: False, ensemble: 1}
  #critic: {layers: 3, units: 1024, act: silu, norm: rms, dist: mse, outscale: 0.0, inputs: [deter, stoch, action], winit: normal, block_fans: False, block_norm: False, ensemble: 64}
  value: {layers: 3, units: 1024, act: silu, norm: rms, dist: symexp_twohot, outscale: 0.0, inputs: [deter, stoch], winit: normal, bins: 255, block_fans: False, block_norm: False}
  #value: {layers: 3, units: 1024, act: silu, norm: rms, dist: mse, outscale: 0.0, inputs: [deter, stoch], winit: normal, block_fans: False, block_norm: False}
  actor_dist_disc: onehot
  #actor_dist_cont: normal
  #actor_dist_cont: trunc_normal
  actor_dist_cont: tanh_normal
  imag_start: all
  imag_repeat: 1
  imag_length: 5 #15
  imag_unroll: False
  horizon: 333
  return_lambda: 0.95
  return_lambda_replay: 0.95
  slow_critic_update: 1
  slow_critic_fraction: 0.02
  retnorm: {impl: perc, rate: 0.01, limit: 1.0, perclo: 5.0, perchi: 95.0}
  valnorm: {impl: off, rate: 0.01, limit: 1e-8}
  advnorm: {impl: off, rate: 0.01, limit: 1e-8}
  actent: 0.0 #3e-4
  slowreg: 1.0
  slowtar: False

size1m: &size1m
  dyn.rssm: {deter: 32, hidden: 32, classes: 32, stoch: 32}
  #actor: {inputs: [deter]}
  #critic: {inputs: [deter, action]}
  .*\.depth: 16
  .*\.units: 128

size3m: &size3m
  dyn.rssm: {deter: 200, hidden: 128, classes: 16, stoch: 16}
  .*\.depth: 16
  .*\.units: 128

size12m: &size12m
  dyn.rssm: {deter: 2048, hidden: 256, classes: 16}
  .*\.depth: 16
  .*\.units: 256

size25m: &size25m
  dyn.rssm: {deter: 3072, hidden: 384, classes: 24}
  .*\.depth: 24
  .*\.units: 384

size50m: &size50m
  dyn.rssm: {deter: 4096, hidden: 512, classes: 32}
  .*\.depth: 32
  .*\.units: 512

size100m: &size100m
  dyn.rssm: {deter: 6144, hidden: 768, classes: 48}
  .*\.depth: 48
  .*\.units: 768

size200m: &size200m
  dyn.rssm: {deter: 8192, hidden: 1024, classes: 64}
  .*\.depth: 64
  .*\.units: 1024

size400m: &size400m
  dyn.rssm: {deter: 12288, hidden: 1536, classes: 96}
  .*\.depth: 96
  .*\.units: 1536

minecraft:
  task: minecraft_diamond
  run:
    log_keys_max: '^log_inventory.*'
  enc.spaces: 'image|inventory|inventory_max|equipped|health|hunger|breath'
  dec.spaces: 'image|inventory|inventory_max|equipped|health|hunger|breath'

dmlab:
  task: dmlab_explore_goal_locations_small
  enc.spaces: 'image|instr'
  dec.spaces: 'image|instr'
  run:
    steps: 2.6e7

atari5p:
  #<<: *size12m
  task: atari5p_pong
  opt.schedule: cosine
  opt.anneal: 1e5 
  run.steps: 1e5
  run.eval_eps: 1
  run.num_envs_eval: 1
  env.atari5p.size: [96, 96]
  (enc|dec).simple.minres: 6
  enc.spaces: 'image'
  dec.spaces: 'image'

atari1p:
  #<<: *size12m
  task: atari1p_pong
  opt.schedule: cosine
  opt.anneal: 1e5 
  run.steps: 1e5
  run.eval_eps: 1
  run.num_envs_eval: 1
  env.atari1p.size: [96, 96]
  (enc|dec).simple.minres: 6
  enc.spaces: 'image'
  dec.spaces: 'image'

atari:
  <<: *size12m
  task: atari_pong
  opt.schedule: cosine
  opt.anneal: 2e5 
  #run.steps: 3e5
  run.steps: 2e5
  #run.eval_eps: 1
  #run.num_envs_eval: 1
  env.atari.size: [96, 96]
  (enc|dec).simple.minres: 6
  enc.spaces: 'image'
  dec.spaces: 'image'

procgen:
  task: procgen_maze-500
  env.procgen.size: [96, 96]
  (enc|dec).simple.minres: 6
  run:
    steps: 1.1e8
  run.eval_eps: 10
  run.num_envs_eval: 10
  enc.spaces: 'image'
  dec.spaces: 'image'

atari100k:
  #<<: *size12m
  task: atari_pong
  run:
    #steps: 3e5 #1.1e5
    num_envs: 1
  enc.spaces: 'image'
  dec.spaces: 'image'

crafter:
  task: crafter_reward
  run:
    num_envs: 1
    log_keys_max: '^log_achievement_.*'
    log_keys_sum: '^log_reward$'
    log_video_fps: 10
    steps: 1.1e6
  enc.spaces: 'image'
  dec.spaces: 'image'

d4rl_proprio:
  <<: *size12m
  #batch_length: 16
  #batch_length_eval: 16
  #batch_size: 256
  #batch_length: 2
  #batch_length_eval: 2
  #run.eval_eps: 10
  #horizon: 100
  #enc.simple.layers: 1
  #critic.layers: 2
  #value.layers: 2
  #actor.layers: 2
  #loss_scales.critic: 1.0 
  #critic.dist: mse
  #value.dist: mse
  opt.lr: 1e-4 
  opt.schedule: cosine
  opt.anneal: 1e5
  task: d4rl_halfcheetah-medium_replay
  #task: d4rl_hopper-medium_replay
  run.steps: 1e5
  enc.spaces: 'vector'
  dec.spaces: 'vector'

dmc_proprio:
  <<: *size50m
  task: dmc_walker_walk
  run.steps: 3e5
  env.dmc.image: False

dmc_vision:
  <<: *size12m
  #imag_length: 5
  #horizon: 100
  #batch_length: 20
  #batch_length_eval: 20
  #batch_size: 64
  #batch_length: 8
  #batch_length_eval: 8
  #<<: *size12m
  task: dmc_walker_walk-medium_replay
  #opt.lr: 1e-4 
  opt.schedule: cosine
  opt.anneal: 1.5e5 #6e5
  run.steps: 1.5e5
  actent: 3e-4
  enc.spaces: 'image'
  dec.spaces: 'image'

bsuite:
  task: bsuite_mnist/0
  run.num_envs: 1
  run.save_every: -1

loconav:
  task: loconav_ant_maze_m
  env.loconav.repeat: 1
  run:
    log_keys_max: '^log_.*'

memmaze:
  task: memmaze_11x11
  enc.spaces: 'image'
  dec.spaces: 'image'

multicpu:
  jax:
    logical_cpus: 8
    policy_devices: [0, 1]
    train_devices: [2, 3, 4, 5, 6, 7]
  run:
    num_envs: 8
    actor_batch: 4
  batch_size: 12

debug:
  task: dmc_walker_walk-medium_replay
  jax: {debug: True, jit: True, profiler: False, checks: False}
  wrapper: {length: 100, checks: True}
  run: {num_envs: 4, eval_every: 10, log_every: 5, save_every: 15, actor_batch: 2, driver_parallel: False}
  report_gradnorms: False
  batch_size: 4
  batch_length: 12
  batch_length_eval: 12
  replay.size: 1e4
  (rewhead|critic).bins: 9
  dyn.rssm: {deter: 12, hidden: 8, stoch: 4, classes: 4, blocks: 4}
  .*\.layers: 2
  .*\.units: 8
  .*\.depth: 2
